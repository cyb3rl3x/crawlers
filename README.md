# Crawlers

Webcrawlers, webscrappers ou spiders são programas criados para navegar entre páginas web, coletando e analisando seu conteúdo.
Os programas aqui listados foram criados para coletar dados de diferentes fontes e gerar de bases para alimentar algoritmos de aprendizagem de máquina.

